{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SwarmAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "    def act(self, state):\n",
        "        # Define rules for agent behavior based on the state\n",
        "        if state[0] > 0.5 and state[1] > 0.5:\n",
        "            action = np.array([1.0])  # Take action 1\n",
        "        else:\n",
        "            action = np.array([0.0])  # Take action 0\n",
        "\n",
        "        return action\n",
        "\n",
        "class SwarmImitationLearning:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.agents = [SwarmAgent(state_dim, action_dim) for _ in range(10)]  # Create 10 swarm agents\n",
        "\n",
        "    def test(self, test_data):\n",
        "        rewards = []\n",
        "        for agent in self.agents:\n",
        "            episode_reward = 0\n",
        "\n",
        "            for state, action in test_data:\n",
        "                state = np.array(state)\n",
        "                action = np.array(action)\n",
        "                predicted_action = agent.act(state)\n",
        "                episode_reward += np.sum(np.square(predicted_action - action))\n",
        "\n",
        "            rewards.append(episode_reward)\n",
        "        return rewards\n",
        "\n",
        "# Example usage\n",
        "state_dim = 2\n",
        "action_dim = 1\n",
        "\n",
        "# Test data (states and actions)\n",
        "test_data = [\n",
        "    ([0.7, 0.8], [1.0]),\n",
        "    ([0.3, 0.6], [0.0]),\n",
        "    ([0.9, 0.2], [1.0]),\n",
        "    ([0.4, 0.1], [0.0])\n",
        "]\n",
        "\n",
        "# Create and test the rule-based imitation learning model\n",
        "il_model = SwarmImitationLearning(state_dim, action_dim)\n",
        "rewards = il_model.test(test_data)\n",
        "print(\"Test rewards:\", rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKSudBzNNGB-",
        "outputId": "9efaa197-4949-451b-a1b7-3199dee2715f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A feedforward neural network can be used as an imitation learning model in swarms."
      ],
      "metadata": {
        "id": "n_D3mICkOPZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FeedforwardNN:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.weights = np.random.rand(input_dim, output_dim)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        return np.dot(input_data, self.weights)\n",
        "\n",
        "class SwarmImitationLearning:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.agents = [FeedforwardNN(input_dim, output_dim) for _ in range(10)]  # Create 10 swarm agents\n",
        "\n",
        "    def train(self, training_data, epochs, learning_rate):\n",
        "        for _ in range(epochs):\n",
        "            for agent in self.agents:\n",
        "                for input_data, target_output in training_data:\n",
        "                    input_data = np.array(input_data)\n",
        "                    target_output = np.array(target_output).reshape((self.output_dim,))\n",
        "                    predicted_output = agent.predict(input_data)\n",
        "\n",
        "                    # Update agent's weights using gradient descent\n",
        "                    gradient = np.dot(input_data.T, (predicted_output - target_output))\n",
        "                    agent.weights -= learning_rate * gradient\n",
        "\n",
        "    def test(self, test_data):\n",
        "        rewards = []\n",
        "        for agent in self.agents:\n",
        "            episode_reward = 0\n",
        "\n",
        "            for input_data, target_output in test_data:\n",
        "                input_data = np.array(input_data)\n",
        "                target_output = np.array(target_output).reshape((self.output_dim,))\n",
        "                predicted_output = agent.predict(input_data)\n",
        "                episode_reward += np.sum(np.square(predicted_output - target_output))\n",
        "\n",
        "            rewards.append(episode_reward)\n",
        "        return rewards\n",
        "\n",
        "# Example usage\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "n12 = np.squeeze(np.asarray(input_dim ))\n",
        "X12 = np.squeeze(np.asarray(output_dim ))\n",
        "\n",
        "# Training data (inputs and targets)\n",
        "training_data = [\n",
        "    ([0, 0], [0]),\n",
        "    ([0, 1], [1]),\n",
        "    ([1, 0], [1]),\n",
        "    ([1, 1], [0])\n",
        "]\n",
        "\n",
        "# Test data (inputs and targets)\n",
        "test_data = [\n",
        "    ([0, 0], [0]),\n",
        "    ([0, 1], [1]),\n",
        "    ([1, 0], [1]),\n",
        "    ([1, 1], [0])\n",
        "]\n",
        "\n",
        "# Create and train the feedforward neural network model\n",
        "il_model = SwarmImitationLearning(input_dim, output_dim)\n",
        "il_model.train(training_data, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "rewards = il_model.test(test_data)\n",
        "print(\"Test rewards:\", rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "xivTw_baN8An",
        "outputId": "5be205d9-6c0c-4480-e729-41e89d50b648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3681b79e11c7>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Create and train the feedforward neural network model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mil_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwarmImitationLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mil_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Test the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3681b79e11c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;31m# Update agent's weights using gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rv9JwReFOKH9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}